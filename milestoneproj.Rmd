---
title: "Milestone Report for the Capstone Project"

output: 
  html_document:
    keep_md: true
---

milestoneproj.Rmd
Prepared by Marcel Merchat
September 4, 2016

##      Title:  Text Prediction Algorithm

##      Synopsis: 

The purpose of this milestone report is to demonstrate some key project issues
such as the following:

- Download the US English data files and import it into the program.

- Provide summary statistics about the data sets.

- Outline a plan for a prediction algorithm and Shiny Application.

```{r load_libraries,echo=FALSE,message=FALSE}
        library(ggplot2)
        library(lubridate)
        library(xtable)
        library(gridExtra)
        options(warn = -1)
  
```

###     Download Data

The data is unzipped into a folder called Data.  

```{r download, echo=TRUE}

setwd("~/edu/Data Science/capstone/Project")

##  fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
##  download.file(fileUrl, dest="zipData.zip")
##  unzip("zipData.zip", exdir=".")

```

###     Summary of Raw Data:
There are folders for four languages. One of the folders is for English and it
has three files of chatting, news, and blogs as follows:

en_US.twitter.txt
en_US.news.txt
en_US.blogs.txt

The chat data consists of over 2 million lines of chat. Similarly there are
about 77,000 lines of news and 900,000 lines of blog text some of which are the
length of journal articles. For the United States, there are about 30,000,000
words in the Twitter data file, about 35,000,000 in the Blog data file, and
about 2.6 million in news file. Below we estimate the true word counts from
the training data sets we construct.

## Line Counts

```{r examinedata, echo=TRUE}

setwd("~/edu/Data Science/capstone/Project")

ustwitter <- readLines("./Data/en_US/en_US.twitter.txt")
length(ustwitter) #[1] 2360148
usnews <- readLines("./Data/en_US/en_US.news.txt")
length(usnews)     #[1] 77259
usblogs <- readLines("./Data/en_US/en_US.blogs.txt")
length(usblogs)    #[1] 899288

```

##      Processing for Training Data Set

```{r mining_function, cache=TRUE, echo=FALSE}

findmatchgreg <- function(text,pattern){
  re <- gregexpr(pattern,text)
  rm <- regmatches(text, re)
  unlist(Filter(function(x) !identical(character(0),x), rm))
}

findmatchexec <- function(text,pattern){
  found <- findmatchgreg(text, pattern) 
  applied <- lapply(found,
                    function(x) {regmatches(x, regexec(pattern,x))[[1]][2]})
  unlist(Filter(function(x) !identical(character(0),x), applied))
}

get_training_set <- function(data, prob, seed){
    set.seed(seed)
    n <- length(data)  
    size <- 1
    inTrain  <- rbinom(n, size, prob)
    inTraining <- inTrain==1
    data[inTraining]
}

get_dictionary <- function(data, minimum_word_count=3, source_col_title){
    pattern <- "[a-zA-Z']{1,15}"
    anyword <- findmatchgreg(data,pattern)
    wordquantity <- length(anyword)
    table1 <- table(anyword)
    sorted <- sort(table1, decreasing=TRUE)
##  popular_dictionary is a named vector of word count
##  where the name is the Word.
    popular_dictionary <- sorted[sorted>=minimum_word_count]
    df <- data.frame(popular_dictionary)
    df[,"Source"] <- source_col_title
    df
}

get_word_count <- function(data){
    pattern <- "[a-zA-Z']{1,15}"
    anyword <- findmatchgreg(data,pattern)
    length(anyword)
}

get_3_gram_count <- function(data){
    pattern <- "[a-zA-Z']{1,18}[ +]{1,2}[a-zA-Z']{1,18}[ +]{1,2}[a-zA-Z']{1,18}"
    any3 <- findmatchgreg(data,pattern)
    length(any3)
}

removefirstword <- function(data){
  pattern <- "^[a-zA-Z']{1,128}\\s(.*)"
  findmatchexec(data, pattern)
  #unlist(Filter(function(x) !identical(character(0),x), withoutfirst))
}

get_2_gram_dictionary <- function(data,
                                  minimum_word_count=10, source_col_title){
  pattern <- "[a-zA-Z']{1,18}[ +]{1,2}{1,2}[a-zA-Z']{1,18}"
  any3 <- findmatchgreg(data,pattern)
  table3 <- table(any3)
  sorted3 <- sort(table3, decreasing=TRUE)
  popular_phrases <- sorted3[sorted3>=minimum_word_count]
  df <- data.frame(popular_phrases)
  df[,"Source"] <- source_col_title
  df
}

get_3_gram_dictionary <- function(data,
                                  minimum_word_count=3, source_col_title){
  pattern <- "[a-zA-Z']{1,18}[ +]{1,2}[a-zA-Z']{1,18}[ +]{1,2}[a-zA-Z']{1,18}"
  any3 <- findmatchgreg(data,pattern)
  table3 <- table(any3)
  sorted3 <- sort(table3, decreasing=TRUE)
  popular_phrases <- sorted3[sorted3>=minimum_word_count]
  df <- data.frame(popular_phrases)
  df[,"Source"] <- source_col_title
  df
}

```

###Word Count Tables for Training Data
We will estimate the number of words in the United States data set directly
below using the training data set.

### Training Data Sets
We considered the AppliedPredictiveModeling package but the us_twitter data
file required too much computer memory. Selecting random lines with a random
TRUE or FALSE vector generated with the R rbinom function worked. Our analysis
and exploration is based on a very small sampling of the data set but it still
includes approximately one million words of text from each of the three files. 

```{r data_Processing, echo=FALSE}
##  Twitter

##  Initial Words Counts for 1% Level of Training Data
    prob <- 0.01
    samples <- 1000000
    twitraining <- get_training_set(ustwitter, prob, seed=333)
    twitter_word_count1 <- get_word_count(twitraining)
  
    blogtraining <- get_training_set(usblogs, prob, seed=333)
    blog_word_count1 <- get_word_count(blogtraining) 
    
    newstraining <- get_training_set(usnews,prob, seed=333)
    news_word_count1 <- get_word_count(newstraining)  
    
##  Equalized Training Dictionaries of 1 million Words
    twitraining <- get_training_set(ustwitter,
                          prob=prob*samples/twitter_word_count1, seed=333)
    blogtraining <- get_training_set(usblogs,
                          prob=prob*samples/blog_word_count1, seed=333)
    newstraining <- get_training_set(usnews,
                          prob=prob*samples/news_word_count1, seed=333)
    
##  Single Word Dictionaries (1-grams)
    twitter_1_grams <- get_dictionary(twitraining,
                                      minimum_word_count=20,"Twitter")
    blog_1_grams    <- get_dictionary(blogtraining,
                                      minimum_word_count=20, "US_Blogs")
    news_1_grams    <- get_dictionary(newstraining,
                                      minimum_word_count=20, "US_News")
    
```

## Word Count Estimates from Training Data
For the United States, there are about 30,000,000 words in the Twitter file,
about 35,000,000 in the Blog file, and about 2.6 million in news file.
```{r wordcountsandtables, echo=TRUE}
##  us_twitter
    get_word_count(twitraining)/(prob*samples/twitter_word_count1)  

##  us_blogs
    get_word_count(blogtraining)/(prob*samples/blog_word_count1)   

##  us_news
    get_word_count(newstraining)/(prob*samples/news_word_count1)   
  
```

## Histograms
The histogram below shows there are many rare words which are not used very
much while only a few words appear many times. To illustrate this, the x-axis
represents the popularity of words and the y axis indicates how many words have
a given amount of popularity.

### Individual Word Frequencies (1-Grams)

```{r blogsandnews_Processing, cache=TRUE, hide=FALSE}

  dfsources <- rbind(twitter_1_grams, blog_1_grams, news_1_grams)
  #dfsources <- twitter_1_grams

  ggplot(dfsources, aes(x=Freq)) + 
  ggtitle("1-Gram Word Count vs Popularity") +
  stat_bin(boundary=20,breaks=seq(20,200,by=10)) +
  coord_cartesian(xlim = c(20, 200)) +
  scale_x_continuous(name="Popularity per Million Words (Occurrence Rate)",
                     limits=c(20, 200)) +
  scale_y_continuous(name="Number of Words") +
  facet_grid(. ~ Source)

```

### 2-Gram Phrases
These histograms show the lower number of popular phrases.

```{r grams2}
    twitter_2_grams <- get_2_gram_dictionary(twitraining,
                                             minimum_word_count=10,"Twitter")
    blog_2_grams <- get_2_gram_dictionary(blogtraining,
                                          minimum_word_count=10,"US_Blogs")
    news_2_grams <- get_2_gram_dictionary(newstraining,
                                          minimum_word_count=10,"US_News") 
    
    dfsources2 <- rbind(twitter_2_grams, blog_2_grams, news_2_grams) 

    ggplot(dfsources2, aes(x=Freq)) + 
    ggtitle("2-Gram Phrase Count vs Popularity") +
    stat_bin(boundary=2.5,breaks=seq(5, 1000, by=5)) +
    coord_cartesian(xlim = c(0, 100)) +
    scale_x_continuous(name=
                  "Popularity of Phrases per Million Words (Occurrence Rate)",
                  limits=c(2.5, 1000)) +
    scale_y_continuous(name="Number of Phrases") +
    facet_grid(. ~ Source)
  
    head(twitter_2_grams)
    head(blog_2_grams)

    nums2 <- as.numeric(as.character(twitter_2_grams$Freq))
    
##  Here is a statistics summary for 2-gram phrases.   
    summary(as.numeric(as.character(twitter_2_grams$Freq)))

```

### 3-Gram Phrases 

```{r grams3}
    twitter_3_grams <- get_3_gram_dictionary(twitraining,
                                             minimum_word_count=3,"Twitter")
    blog_3_grams <- get_3_gram_dictionary(blogtraining,
                                          minimum_word_count=3,"US_Blogs")
    news_3_grams <- get_3_gram_dictionary(newstraining,
                                          minimum_word_count=3,"US_News") 
    dfsources <- rbind(twitter_3_grams, blog_3_grams, news_3_grams) 

    ggplot(dfsources, aes(x=Freq)) + 
    ggtitle("3-Gram Phrase Count vs Popularity") +
    stat_bin(boundary=2.5,breaks=seq(2.5,200.5,by=1)) +
    coord_cartesian(xlim = c(0, 23)) +
    scale_x_continuous(name=
                  "Popularity of Phrases per Million Words (Occurrence Rate)",
                  limits=c(0, 300)) +
    scale_y_continuous(name="Number of Phrases") +
    facet_grid(. ~ Source)
  
    head(twitter_3_grams)
    head(blog_3_grams)

    nums3 <- as.numeric(as.character(twitter_3_grams$Freq))
##  Here is a statistics summary for 3-gram phrases.  
    summary(as.numeric(as.character(twitter_3_grams$Freq)))

```

##  Word Prediction Stategy 

Consider the string "Nothing in the world." The last three words are
"in the world." Since for our problem we will not know what the final
word is, we start with the string "Nothing in the" and use the last two words
which are "in the" to predict the unknown final word.
 
#### Find the last known word:
```{r lastword}

str <- "Nothing in the"
patternlast <- "\\s([a-zA-Z']{1,50})[\\.]?$"
re <- regexec(patternlast,str)
lastword <- regmatches(str, re) [[1]][2]
lastword

```

#### Find the next to last Word:
The last two words form a 2-gram object, but here we only consider what might be
predicted from the 3-gram model given that we know the first two words are
"in the." 

```{r nexttolastword}
patternnexttolast <- "\\s([a-zA-Z']{1,50})\\s[a-zA-Z']{1,50}[\\.]?$"
re <- regexec(patternnexttolast,str)
nexttolast <- regmatches(str, re) [[1]][2]
nexttolast

```

##   Goals for Shiny App and Algorithm

### The 3-gram Model Prediction:

N-gram model for predicting the next word based on the previous 1, 2, or 3 words.
We will also attempt to handle unseen n-grams too. We form a data frame that
is suitable as input for a prediction model in the Carot Package. The first word
of the 2-gram model might somehow be added to the same data frame but here we
simply see that there a few phrases in our 3-gram data that we can choose a
correct response from. Perhaps the most likely choice would be the 3-gram with
the highest frequency of occurrence.

```{r }
    col1 <- unlist(lapply(as.character(twitter_3_grams[,1]),
                          function(x) {strsplit(x, "\\s+")[[1]][1]}))
    col2 <- unlist(lapply(as.character(twitter_3_grams[,1]),
                          function(x) {strsplit(x, "\\s+")[[1]][2]}))
    col3 <- unlist(lapply(as.character(twitter_3_grams[,1]),
                          function(x) {strsplit(x, "\\s+")[[1]][3]}))
    data <- data.frame(col1, col2, col3, stringsAsFactors =FALSE)

##  Given 2-gram or two word combination: Find possible matching 3-gram from
##  the Twitter data set.
    paste(nexttolast,lastword)
    
##  Find possible matching 3-gram from the Twitter data set.
    data[data[,1]==nexttolast & data[,2] == lastword,]
```

#### THE END





